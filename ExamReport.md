1. Introduction
This report describes the efforts spend on the construction of the Hackernews clone system. The aim is to give an overview of the considerations behind the system, from an angle balancing somewhere in between the theoretical and technical boundaries. The report consists of two parts: One describing the process of implementation, and the other describing how the system is thought out to be maintained.      
1.1. System requirements
The original Hackernews application is a web application much similar to www.reddit.com, that primarily facilitates links to news articles served by other websites. The users can also share job adds, announcements, and debatable questions to the site - either as links to other sites or within the Hackernews site itself. All such posts are referred to as Items. A key feature that  applies for all posted Items, is the users ability to comment on them, or even on other comments (as are in fact considered Items themselves). The users are also able to up-vote any Item they may find worthy of acknowledgment. Through such votes given to their Items, users are able to gather so-called Karma Points, which they in turn can use to accredit other users contributions via up-votes. Since these features requires some degree state management, a simple user control functionality has been implemented. A User just needs to supply a username and a password, in order to create an account and join the community. The original Hackernews site can be found at: https://news.ycombinator.com/. 
The Hackernews clone (which will from heron be referred to as Hackernews) is an imitation of the of the application described above, that implements only the core functionality. In terms of functional requirements, the users should thereby be able to execute the following use cases:
Create Account.
Login to Account. 
Logout of Account. 
Post Item.
Post Comment.
View Items. 
View Comments. 
Additionally it was decided, that users should also be able to access the system via a REST endpoint and:
Post Items.
Obtain System Status.
Obtain Id of Last Item Persisted.
Naturally the Hackernews users should be able to accomplish all of the above mentioned use cases via a web browser (or REST endpoint). 
With respect to the FURPS principal, performance and reliability has been the main focus for this project. The goal was to implement a system that would be able to handle high throughput. That is, millions of user transactions over a period of a few months. A vast amount of Item posts were expected to occur immediately after system deployment, hence the choice of technology and infrastructure had to accommodate this challenge, without compromising up-time or cause latency issues for the users. It was thereby the ambition to deliver a system with:
High up-time.
Ability to handle high throughput.
A tolerable latency for user requests submitted via the website.
Furthermore it became clear, that some degree of redundancy and scalability on key components were needed, in order to achieve the performance and reliability requirements. Therefor the following requirements were added to the specification:
The application must incorporate the ability to monitor critical functionality.
The application must support tools for logging and analyzing system data.
The application must be able to scale both horizontally and vertically.
Although performance and reliability were the main areas of focus for the Hackernews project, concerns was also given to supportability – namely maintainability and and repair speed. As such, the following requirement emerged regarding repair-speed:
Application updates must be pushed to production within a tolerable time threshold. 
This requirement should only concern adjustments or bug-fixes to the code, since major updates tend to require some level of preparation towards the users – either as a company announcement or a simple warning, that a considerable change is about to happen to the system.
In order to increase maintainability, decisions were made to use well known technologies and platforms, wherever possible. That way experience gained by other developers and communities could be utilized, and a larger number of potential future developers would be able to provide support and maintenance to the system. It was also the goal that these technologies should be platform (OS) agnostic, and preferably open source. Furthermore it was the ambition to strive for the use of well known software architectures and design patterns, in order to achieve a high level of portability for the system. Altogether these choices should reduce the risk of vendor locking, and support the ambition of high scalability and redundancy, since application instances would be able to run on almost any kind of server platform. In accordance with these objectives, the following non-functional requirements were finally added:
The system should preferably be platform agnostic.
The system must be developed using well known technologies and platforms.
The system must implement well known architectural- and design patterns.
The system should utilize open-source technologies wherever possible.
1.2. Development process
Since all of the functional requirements for the system could be derived from the original Hackernews application, it seemed obvious to choose a disciplined development methodology. However other parameters suggested an agile approach: Using the axis' on Boehm's spider web to determine a suitable development methodology, it was clear that all of the values, except for dynamism, could be plotted somewhere near the center of the model. In the illustration below the project properties are outlined with a red polygon:

Although Boehm's spider web dictates a mainly agile approach for such a project, it was decided to instantiate a lightweight version of Unified Process (UP) instead. Since Hackernews is a replica of an already existing application, all system requirements were easily identified up front. It therefor seemed sensible to translate them into use cases, and then work out a few suitable UML models as baseline for the actual development process.
In the Inception phase a simple vertical prototype, reaching from the presentation tier to the persistence layer, would be build to serve as a proof of concept. This way it could be established whether the choices of development platforms would stand up to the functional requirements. A simple UML model depicting this prototype would be worked out up-front. Altogether these artifacts would make out the candidate architecture for the Hackernews application. 
The Elaboration phase would be used to refine the prototype from the Inception phase, making it fully functional and testable. The previously designed UML diagram would now be refined and expanded, so that nearly all requirements were documented at this point. Additionally all the base elements of the continuous delivery chain would be setup, to determine that no incompatibilities between the development platforms and deployment technologies would cause problems. With this in mind, it would of cause be crucial, that the vertical prototype provided as much coverage of the system functionality as possible, to avoid finding out later on, that key elements, such as password and connection configurations would cause problems.   
In the Construction phase the remaining implementation would take place and gradually be rolled out into production, as part of the continuous delivery chain. As such this phase would be somewhat merged together with the Transition phase, giving the entire development process a small accent of an agile approach.         
1.3. Software architecture
In accordance with the non-functional requirement regarding the use of well known software architecture, it was decided to use a Three-Tier architectural scheme for the Hackernews application itself. Hence, the application should be designed to comprise three different layers (or tiers): the presentation tier, the application tier, and a data tier. All communication between the different modules happens in a linear fashion in this architecture. That means, the presentation tier, which handles the UI,  communicates with the application tier, were all the business logic occurs, which in turn communicates with the data tier, were data is persisted. This architecture has proven its worth in numerous applications, and provides a transparent way to segment the different modules of the application. This approach was to be used for the application as the overall architecture. The figure below illustrates the Three-Tier architecture:

The frontend module (the presentation tier) should implement a Model View Control (MVC) architecture. This is a common practice that many frameworks implements out of the box, and as such, this decision supported the requirement of using well-known technologies, to leverage high maintainability. Unlike the Three-Tier architecture this setup can handle communication in a triangular fashion. E.g. the view communicates with the control layer which contacts the model, which then sends the respond back directly to the view layer. In some cases the view can also contact the model directly without involving the controller. The rest of the application should also implement this pattern, with respect to the delegation of responsibility and readability. The MVC architecture is illustrated below:

At the persistence tier it was decided to initially use a relational database as primary persistence device. This choice was primarily based on the ambition about using well-known technologies wherever possible. However the application should preferably be able to accommodate alternative storage solutions, as it could turn out, that other platforms, such as NoSQL databases would prove more beneficial in terms of performance and scalability. It was therefor decided, that the Hackernews application should be as database agnostic as possible. 
With respect to the requirement of having updates pushed to production within a tolerable time limit, it was decided, that the development life cycle should be incorporated into a continuous delivery chain. In order to establish such a set up, certain architectural considerations with regard to the system infrastructure had to be made. It was decided, that the chain as a minimum should include: A source code repository that can handle version control, a build server supporting automation, and a platform able to containerize, and preferably scale all components comprising the application. Additionally an Artifactory platform could be inserted if needed. The figure below illustrates the desired architectural setup for the deployment chain:

The production server shown in figure above handles two containers that encapsulates the application as a whole. This is to illustrate the idea of scalability via containerization. Theoretically this number could be greater depending on the hardware resources available on the machine(s), but of course, at least one container should always be present.      
1.4. Software design
For the implementation of the Hackernews application it was decided to use Java, namely Java EE, as language and platform. The frontend would be worked out using the Primefaces Java Server Faces (JSF) library, and for the backend Java Persistence API (JPA) and Enterprise Java Beans (EJB) would be used. The application build should be deployed to a GlassFish 4 server, which would run on an Amazon EC2 Linux cloud server. GlassFish provides the opportunity to add all server configurations via command-line scripting, a feature deemed beneficial in terms of continuous deployment and delivery. PostgreSQL was chosen as database platform. All of these technologies and platforms abide the ambition of utilizing widely propagated technologies, and support the previously made architectural decisions.
The application would be implemented using a bottom-up approach by starting with the construction of a suitable relational schema, that could accommodate the requirements for data persistence. It was also decided to implement the application as a single package (one .war file), instead of splitting it into multiple modules/packages. The design should however offer an opportunity to segment out frontend or backend modules into standalone packages, if needed at a later point. Since Hackernews is a fairly simple application – at least from a database perspective – only two relations were needed: a table for users and a self-referencing table for Items. With the relational schema in place, a JPA object model reflecting the data-tables' content and relationships could be worked out, along with an EJB facade handling connectivity to the database. It's worth mentioning, that JPA provides a substantial abstraction from the data tier in the form of low coupling, and – if used appropriately – makes the overlying application somewhat database agnostic. This technology is thereby chosen in full accordance with architectural decisions about not committing the system to one database vendor (making it database agnostic).   
The frontend of Hackernews would be implemented using the before mentioned MVC design, leveraged by JSF. In this setup all HTML and JavaScript is rendered in Facelets (a file with .xhtml extension) and forwarded to the client per request. These files reside in the View layer in the JSF MVC scheme, and communicate with so-called Backing Beans, which make out the Model. All requests made from the client (i.e. the browser) are sent to a Servlet, called the Faces Servlet. This class constitutes the Controller. Although the Backing Beans are considered Model entities, from a frontend perspective, they also function as a facade against the business layer. As such they have a dual role in the application. Below is an illustration outlining the desired design for the Hackernews Java implementation:

Referencing the continuous delivery chain diagram, it was decided to use Git as code repository and version control system. This platform was mainly chosen because of its wide propagation, but also because it offers flexibility in terms relocating the repository. 
For application packaging Docker was chosen. This platform provides the means of containerizing almost any application and its surrounding runtime environment in a somewhat transparent fashion. This is achieved through virtualization, where Docker acts as a lightweight Hypervisor, residing on top of the host OS. Each container, pushed to the production server, includes a Linux distribution where the application environment is configured and deployed. Docker also offers the opportunity of high scalability and redundancy through the Docker Swarm feature, which allows the developers and operators to instantiate multiple containers – either over a network of serves, or on a single machine. 
Since Hackernews mainly relies on standard Create, Read Update, Delete (CRUD) functionality, and from a developer's point of view doesn't contain much specialized code, an Artifactory was deemed unnecessary. In other words, the amount of application source code and its complexity, didn't call for an such a platform. However it was decided to use Maven as build tool, along with its public code repository, for all standard libraries needed in the application.
Jenkins was chosen as build server because of its excellent support for Java applications (Jenkins is in fact a Java application itself), and because it offers a vast variety of plugins, including support for Docker and Maven. With these decisions in place the continuous delivery chain could be depicted as below:

1.5. Software implementation
With a few exceptions the implementation went as planed. The architecture was implemented according to the requirement specification and the design followed all decisions made in terms of technological choices an use of patterns. Hackernews was implemented using the Java EE stack and PostgreSQL as data source. The application was deployed as a single package via the desired deployment change, described above. 
However a few requirement changes emerged during the end of the process: It was decided that the application should support the insertion of multiple Items on a specified data format, via a RESTful POST request. This format would contain a sequence generated value called hanesst_id, used to identify the Item at hand on the users side. Via another REST endpoint the users should be able to obtain the latest hanesst_id, in order to check how many Items the Hackernews application had consumed at any given point. Finally a REST service should be implemented, providing the user an opportunity to check whether the application was alive or not.
To accommodate the data format for insertion of multiple Items, a class able to bind these newly specified values, was added to the model. Additionally the Item relation in the database, was updated to include the hanesst_id, giving the application a way of persisting and retrieving this value per request. Finally a JAX-RS RESTful resource was added to the application to provide the necessary endpoints. As such the users were now able to persist Items, get the id of the last persisted Item, and check the application status. 
The continuous delivery chain was established in almost complete accordance with the architectural planning. This process proved quite cumbersome in terms of configuration issues, but through trial and error it eventually succeeded. The plan of having full automation of the process was however changed, so that the build and deployment part was handled by the push of a button via the Jenkins server interface. This approach gave more control and a sense of security, since a source code push to the Git repository didn't automatically result in a full deployment cycle. Having the ability to share modifications on Git without further triggering further steps, was thought beneficial to the development process. 
Hence, the deployment chain worked as the following: Changes made to the source code were implemented and tested locally, then pushed to the remote Git repository, the build- and deployment job was manually triggered via the Jenkins interface, and finally packaged via Docker and deployed to the production environment.
The Jenkins server was however not installed on a remote server, but instead deployed to a local machine using Virtual Box and Vagrant to simulate an independent build machine. Through a Vagrant script and a provisioning script this machine was set up and configured to include all necessary platforms and tools in order to run Jenkins. From here an SSH tunnel was established to the remote deployment server, allowing Jenkins to access this environment. Furthermore Jenkins was given credentials enabling it to access Docker Hub in order to push images hereto. More accurately Jenkins:
Pulls the source code from the Git repository, which contains a Dockerfile, and compiles the code locally into a .war file.
It then builds a Docker image containing the newly build .war file and GlassFish applications server, and pushes it to Docker Hub.
It finally evokes a shell script on the production server – which has been placed there in advance –  that pulls the Docker image from Docker Hub and deploys the container via the docker run command.    
2. Maintenance and SLA status
This section describes the process of maintaining the software over time, and the tools involved in that task.
2.1 Service-level agreement 
During the initial development process for the Hackernews application it was decided, that the system should fulfill certain non-functional requirements. Among these it was stated, that the system should have:  
High up-time.
A tolerable latency for user requests submitted via the website.
The system must be able to handle high throughput.
And that:
Application updates must be pushed to production within a tolerable time threshold. 
These ambitions all call for measurable parameters, that can be used to determine how well the requirements are met. Such parameters are called Key Performance Indicators (KPI's). For the requirements stated above system up-time, system latency, data throughput, and maintenance time can be viewed as KPI's. These values are used as foundation for the Service Level Agreements (SLA's), that the Hackernews system should abide. As such, the SLA for Hackernews include the following clauses:
The system must have no less than 95% up-time.
The system must have a latency of no more than 6 seconds for any user actions submitted via the website.
The system must be able to persist no less than a 150.000 data sets pr. day. 
Application updates must be pushed to the production environment in no less than 10 minutes.    
For an external part (e.g. users, customers etc) it's difficult to check whether the last SLA is met. It should therefor be considered an internal SLA, that DevOps personnel must ensure is obtained. However external entities can at least expect, that minor updates, like content changes to the site etc, won't be postponed with the excuse, that the deployment process is causing the delay.         
2.2 Maintenance and reliability
To ensure the performance requirements and SLA's were met, it was decided to include measures to the system that could record and analyze application telemetrics. Such tools should enable operators to monitor the system performance/availability, and also provide developers an opportunity to test and analyze application updates in a pre-production environment. It was decided to user Prometheus and Grafana for monitoring, and the Elasticsearch-, Logstash- and Kibana tools (the ELK stack) for logging data.
The Hackernews source code was instrumented to provide data to Prometheus – a key-value database used to store application telemetrics. These values could then be pulled into Grafana – a dashboard web application used to visualize/monitor the Prometheus output data. This tool offers the ability to construct graphs, histograms etc, depicting the state of the application at hand, and set up thresholds and notifying alarms if any of these are exceeded.  
Prometheus and Grafana were installed locally via Docker images. A .yml file was used to configure Prometheus and point it to the Hackernews REST service, that provided the output data. Grafana was configured via the HTML interface to utilize the Prometheus server as data source. 
For the Hackernews application, the source code was fitted with so-called time-stamp counters in the methods that handle the creation of Items ( for both stories and comments). These counters were placed at the beginning and end of each method, and were thereby able to record the execution time spend for these actions. The results were then exposed via a RESTful endpoint, enabling Prometheus to pull (scrape) these metrics. The data could finally be visualized in Grafana, thereby providing the DevOps an opportunity to perform real-time monitoring and setting up thresholds for critical operations. Namely checking how much time the system consumes when persisting Items. 
However this set up only provided a means for determining the duration of these CRUD operations from an internal point of view. Although this information gave a picture of the servers performance, it didn't offer a way to check whether the latency on the client slide lived up to the SLA. To achieve this, an external job should be implemented, that would provide data to Prometheus/Grafana about the latency – e.g by using the ICMP protocol to send ping requests to the server and measure the round trip duration. Such job was regrettably never implemented for Hackernews.
The source code was also configured to deliver log data to the ELK stack. This was achieved by adding so-called appenders to the code via the Log4j framework. The recorded log data was then sent to the Logstash tool, which in turn forwarded it to the Elasticsearch/Lucene from where it could be visualized in the Kibana web application. This set up allowed for DevOps to do retrospect analysis' – including postmortem reports – on all data recorded in the server- and application logs. 
With regard to the SLA's, the ELK stack could be used to determine if something went wrong and why. Additionally the tool could offer a way to improve the Hackernews application and its runtime environment, by giving information about how the system was used by the users. For instance, if the majority of visitors only read Stories rather than posting them, up-scaling of the frontend modules, with access to read-only data sources could be added. Likewise if the users primarily reads certain Stories, a cache scheme could be rolled out, thereby bypassing communication to the application server(s).        
3 Technical discussion
As an overall, the implementation of the Hackernews source code went as expected. The development process yielded a good traceability between the initial requirements, the architectural decisions, the chosen design, and the end-product.  
However the database was neither incorporated into the continuous delivery chain, or containerized. Instead it was deployed to an Amazon server as a standalone RDS service. This setup poses a problem to the ambition of having a scalable environment – i.e. many containers can be instantiated to increase availability of the application- and presentation tier, but these don't include the database. As such bottleneck situations will occur, if a large enough amount of requests are made to the data tier. For now the only way to mitigate this problem, is to scale the database server vertically, i.e. by increasing its memory- and CPU resources. PostgreSQL does offer way to scale the database horizontally by adding more servers to the setup, but such scheme was never implemented for the Hackernews system.     
The continuous delivery chain proved to be somewhat tedious in terms of setting up- and configuring it. However it's the impression, that time sacrificed on this process, is well spend. Compressing all the pain of building, testing, and deploying an application into one initial task, will offer a fast and reliable way to push application updates to production. 
Although it has a steep learning curve, Docker also proved to be very useful. The flexibility it offers via the ability to run containers on almost any platform, is highly desirable, in terms of system portability. It also provides a high degree of scalability and redundancy for the containerized applications, through the Docker Swarm tool.     
Both Prometheus/Grafana and the ELK stack seem like powerful tools for utilizing application telemetrics and log-data. Although Hackernews was never submitted to any real data insertions, it's the impression, that these tools can be very beneficial not only to the DevOps staff, but for almost any employee in an organization. E.g. displaying the Grafana dashboard on monitors in an office environment, will offer the staff, ranging from the CEO to the cleaning aid, an opportunity to check if everything is running as it should. Since ensuring that a gauge or meter isn't exceeding into the “red zone” is something most people can relate to, instructions on how to mitigate if that happens, can be given to everyone.
A possible drawback of the overall setup for the Hackernews system, is the vast inclusion of different technologies, tools, and platforms. As no chain is stronger than the weakest link, a problem could occur if one the components should become obsolete, or in other ways incompatible with the rest of the system. However the use of widely propagated technologies etc. will offer some insurance to this situation, as many others are likely to face the same difficulties. The larger a community backing up a technology is, the more help is usually available.  
